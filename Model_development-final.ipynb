{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from numpy.fft import fft, fftfreq\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "- test prediction segment accuracy as a function of segment size (todays task. make an argument about physical significance of 136 or 150)\n",
    "- Number of NN layers\n",
    "- Number of layer nodes\n",
    "- Run predictions using frequency domain info (psd) as opposed to time domain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "Label the data:\n",
    "Normal: label #1\n",
    "Inner race fail: label #2\n",
    "Outer race fail: label #3\n",
    "Roller defect: label #4\n",
    "\n",
    "Data includes 3 datasets were created. At the end of the test, the followign was reported:\n",
    "\n",
    "                  |        |   bearing1   bearing2   bearing3   bearing4|\n",
    "                  |--------|--------------------------------------------|\n",
    "                  |        |  Sensor1       sensor2   sensor3   sensor4 |\n",
    "                  |--------|--------------------------------------------|\n",
    "                  |Set #1  |    1             1          2          4   |\n",
    "                  |--------|--------------------------------------------|\n",
    "                  |Set #2  |    3             1          1          1   | \n",
    "                  |--------|--------------------------------------------|  \n",
    "                  |Set #3  |    1             1          3          1   | \n",
    "                  |--------|--------------------------------------------|\n",
    "                  \n",
    "Datasets used for model development:\n",
    "\n",
    "         lable 1: set 2, bearing 2\n",
    "         label 2: set 1, bearing 3\n",
    "         label 3: set 3, bearing 3\n",
    "         label 4: set 1, bearing 4\n",
    "                  \n",
    "                  \n",
    "We assume the above status was valid for that last 20 data collections.\n",
    "Therefore, we get data from last 20 files, and label them as above.\n",
    "\n",
    "The form of training and testing data is a pd dataframe:\n",
    "\n",
    "Columns: set_np, file, seg_no, bearing_no, sensor_outpt, label\n",
    "Each record is a segment.\n",
    "data in sensor_outpt are list of 20480/seg for each classification.\n",
    "seg_no starts is from 1 to seg, for each file\n",
    "bearing_no is the bearing number that the label applies to.\n",
    "label indicates the classification of each bearing health from 1 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(file, set_no, bearing_no, label, domain='time'):\n",
    "    ''' Return the X (features) and y (label) for model prediction\n",
    "    file: the file name\n",
    "    **src is a dic and includes set_no, bearing_no and label\n",
    "    domain corresponds to X and can be time or freq. \n",
    "        \n",
    "    '''\n",
    "    file_path = get_path(set_no, file)\n",
    "    f = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
    "    seg_size = int(f.shape[0] / seg_num)\n",
    "    X_cols = [i for i in range(seg_size)]\n",
    "    # Trim the data in set#1 to make one sensor per bearing\n",
    "    if set_no == '1':\n",
    "        f.drop(f.columns[[1, 3, 5, 7]], axis=1, inplace=True)  \n",
    "    X = pd.DataFrame(columns=X_cols)\n",
    "    y = pd.Series()\n",
    "    for seg_no in range(seg_num):\n",
    "        min_indx = seg_no * seg_size\n",
    "        max_indx = (seg_no + 1) * seg_size \n",
    "        col_num = bearing_no - 1\n",
    "        if domain == 'time':\n",
    "            X_temp = f.iloc[min_indx:max_indx, col_num]\n",
    "            X_temp = pd.Series(X_temp)\n",
    "            X_temp.index = X_cols\n",
    "        # To convert X into freq domain:\n",
    "        else:\n",
    "            X_temp = f.iloc[min_indx:max_indx, col_num]\n",
    "            fft_vals = fft(X_temp)\n",
    "            n = X_temp.size\n",
    "            psd = 2*np.abs(fft_vals/n)**2\n",
    "            X_temp = pd.Series(psd)\n",
    "            #X_temp.index = \n",
    "        X = X.append(X_temp, ignore_index=True)\n",
    "        y = y.append(pd.Series([label]))\n",
    "    return X, y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(set_no, file):\n",
    "    ''' Return full path to the file\n",
    "    set_no is the dataset number (1,2, or 3)\n",
    "    file is the file name (e.g. '2003.10.22.12.39.13')\n",
    "    \n",
    "    '''\n",
    "    path = os.path.join(datasets_dir, set_no, file)\n",
    "    return path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(set_no, files_num=0):\n",
    "    ''' Return the names of the last file_num files in the dataset set_no\n",
    "        if files_num is 0, then it returns all files in the directory\n",
    "        otherwise, it returned files_num number of files\n",
    "    '''\n",
    "    file_dir = os.path.join(datasets_dir, set_no)\n",
    "    all_files = os.listdir(file_dir)\n",
    "    if files_num == 0:\n",
    "        files_num = len(all_files)\n",
    "    all_files.sort(reverse=True)\n",
    "    latest_files = all_files[0:files_num]\n",
    "    return latest_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_accuracy(y_pred, y_test):\n",
    "    ''' Return 4 measures of accuracy based on analysis of a single segment.\n",
    "    Returns: [accuracy of correct prediction of label1, ..., accuracy of correct prediction of label 4]\n",
    "        \n",
    "    '''\n",
    "    ac=[]\n",
    "    for label in range(1, 5):\n",
    "        y_pred_bool= y_pred == str(label)\n",
    "        y_test_bool= y_test == str(label)\n",
    "        accuracy = (y_pred_bool == y_test_bool).mean()\n",
    "        ac.append(accuracy)\n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y):\n",
    "    ''' Return accuracy of the model (from 0 to 1).\n",
    "    y is a list of (y_prediction, label). Each element of the list corresponds to one file\n",
    "    \n",
    "    '''\n",
    "    correct_pred = [int(pred) == label for pred, label in pred_lst]\n",
    "    accuracy = sum (correct_pred)/len(pred_lst)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(set_no, bearing_no):\n",
    "    ''' Finds label (1, 2, 3 or 4) for each of the four bearings, based \n",
    "    on the dataset number.\n",
    "    Return the element in the label list that corresponds to the bearing of interest\n",
    "    \n",
    "    '''\n",
    "    if set_no == '1':\n",
    "        label=[1, 1, 2, 4]\n",
    "    if set_no == '2':\n",
    "        label=[3, 1, 1, 1]  \n",
    "    if set_no == '3':\n",
    "        label=[1, 1, 3, 1]\n",
    "    res = label[bearing_no - 1]\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overal_pred(X, clf):\n",
    "    '''Calculate the overal probability for each class, and return the class with max probability\n",
    "    \n",
    "    '''\n",
    "    prob_seg = clf.predict_proba(X) \n",
    "    prob_overal = prob_seg.sum(axis=0)\n",
    "    max_prob = max(prob_overal)\n",
    "    maxindx = prob_overal.argmax()\n",
    "    classification = maxindx + 1\n",
    "    return classification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_all_sets(domain):\n",
    "    ''' Return the prediction for all files_no files in all sets\n",
    "    retuns a list of (prediction, actual label) for every files used.\n",
    "    domain can be time or freq\n",
    "    '''\n",
    "    pred_lst=[]\n",
    "    files_no = 20\n",
    "    for set_no in ['1', '2', '3']:\n",
    "        files = get_files(set_no, files_no)\n",
    "        for file in files:\n",
    "            for bearing_no in [1, 2, 3, 4]:\n",
    "                label = get_label(set_no, bearing_no)\n",
    "                src = src = {'set_no': set_no, 'bearing_no': bearing_no, 'label': label}\n",
    "                X, y = get_xy(file, **src, domain=domain)\n",
    "                pred = overal_pred(X, clf)\n",
    "                pred_lst.append((pred, label))\n",
    "    return pred_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = 'C:\\\\Users\\\\10134838\\\\Desktop\\\\PProjects\\\\Sensors\\\\Bearing_Cincinati\\\\'\n",
    "seg_num = 136\n",
    "feature_size = 20480 // seg_num\n",
    "X = pd.DataFrame()\n",
    "y = pd.Series()\n",
    "# Number of files from each dataset for segmentation (e.g. files_num = 2 means the last two files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(domain, n_hidden_layers, training_file_no):\n",
    "    ''' Specify these four groups as sources of data\n",
    "             lable 1: set 2, bearing 2\n",
    "             label 2: set 1, bearing 3\n",
    "             label 3: set 3, bearing 3\n",
    "             label 4: set 1, bearing 4\n",
    "             domain can be time or freq\n",
    "    '''\n",
    "    # files_num is the number of files used in each set to train model\n",
    "    src1 = {'set_no': '2', 'bearing_no': 2, 'label': '1'}\n",
    "    src2 = {'set_no': '1', 'bearing_no': 3, 'label': '2'}\n",
    "    src3 = {'set_no': '3', 'bearing_no': 3, 'label': '3'}\n",
    "    src4 = {'set_no': '1', 'bearing_no': 4, 'label': '4'}\n",
    "    src_all = [src1, src2, src3, src4]\n",
    "    # Get the dataframe of data for testing and training\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.Series()\n",
    "    for src in src_all:\n",
    "        files = get_files(src['set_no'], training_file_no)\n",
    "        for file in files:\n",
    "            X_temp, y_temp = get_xy(file, **src, domain=domain)\n",
    "            X = X.append(X_temp)\n",
    "            y = y.append(y_temp)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=1)\n",
    "    clf = MLPClassifier(hidden_layer_sizes=n_hidden_layers)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    seg_ac = sum(seg_accuracy(y_pred, y_test))/4\n",
    "    return seg_ac, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predictions\n",
    "We will investigate two way of testing the model:\n",
    "first, test on segments that have not been used for training. \n",
    "In simplest form, prediction is based on a single segment. \n",
    "For now, lets stick to prediction based on single segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above predictions are for each segments. Below, we get the overal prediction:\n",
    "\n",
    "Overal propability = sum of all segment probabilities\n",
    "Overal prediction corresponds to the class with maximum overal probability\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 100)\n",
      "(100, 50)\n",
      "(100, 50, 25)\n",
      "(100, 50, 25, 12)\n",
      "(100, 100, 50, 25, 12)\n",
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10134838\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 50)\n",
      "(100, 50, 25)\n",
      "(100, 50, 25, 12)\n",
      "(100, 100, 50, 25, 12)\n",
      "(100,)\n",
      "(100, 100)\n",
      "(100, 50)\n",
      "(100, 50, 25)\n",
      "(100, 50, 25, 12)\n",
      "(100, 100, 50, 25, 12)\n",
      "(100,)\n",
      "(100, 100)\n",
      "(100, 50)\n",
      "(100, 50, 25)\n",
      "(100, 50, 25, 12)\n",
      "(100, 100, 50, 25, 12)\n"
     ]
    }
   ],
   "source": [
    "domains = ['time', 'freq']\n",
    "training_file_nos = [10, 20]\n",
    "seg_nos = [136]\n",
    "ns_hidden_layers = [(100,),(100, 100), (100,50), (100,50,25), (100,50, 25, 12), (100,100, 50, 25, 12)]\n",
    "pred_df_cols = ['training_file_no', 'seg_no', 'domain', 'n_hidden_layers', 'seg_ac', 'overal_ac']\n",
    "# pred_df is a df containing on the predictions and the parameters\n",
    "pred_df = pd.DataFrame(columns=pred_df_cols)\n",
    "for training_file_no in training_file_nos:\n",
    "    for seg_no in seg_nos:\n",
    "        for domain in domains:\n",
    "            for n_hidden_layers in ns_hidden_layers:\n",
    "                print(n_hidden_layers)\n",
    "                seg_ac, clf = train_test(domain, n_hidden_layers, training_file_no)\n",
    "                pred_lst = pred_all_sets(domain)\n",
    "                overal_ac = get_accuracy(pred_lst)\n",
    "                record = pd.Series([training_file_no, seg_no, \n",
    "                                    domain, n_hidden_layers, seg_ac, overal_ac], index=pred_df_cols)\n",
    "                pred_df = pred_df.append(record, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "    - Segment accuracy good indicator of overal accuracy\n",
    "    - Best results was overal prediction of 82% which was achieved in frequency domain with 2 hidden layer NN (100,50)\n",
    "    - All errors were false positive, which is more acceptable to industry\n",
    "    - To improve results, add more sensors or train each sensor seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
